embedding_size: 64              # (int) The embedding size of users and items.
train_batch_size: 2048
#margin: 0.5                     # (float) The margin to filter negative samples. Range in [-1, 1].
#negative_weight: 10             # (int) Weight to balance between positive-sample and negative-sample loss. 
gamma: 0.2                      # (float) Weight for fusion of user' and interacted items' representations. large gamma induce more user rep.
aggregator: 'user_attention'              # (str) The item aggregator ranging in ['mean', 'user_attention', 'self_attention'].
history_len: 50                 # (int) The length of the user's historical interaction items.
#reg_weight: 0               # (float) The L2 regularization weights.
weight_decay: 1e-6
train_neg_sample_args:          # (dict) Negative sampling configuration for model training.
  distribution: uniform         # (str) The distribution of negative items.
  sample_num: 1                 # (int) The sampled num of negative items.
  alpha: 1.0                    # (float) The power of sampling probability for popularity distribution.
  dynamic: False                # (bool) Whether to use dynamic negative sampling.
  candidate_num: 0

## Edit From DreamRec
diffuser_type: mlp1
loss_type: l2
timestep: 1                    # 200, diffusion steps
uncon_w: 0                    # 2, the weight of conditioned diffusion in inference phase
uncon_p: 0.1                    # 0.1, how much prob does train phase use unconditioned diffusion
beta_sche: exp                  # exp, the schedule of beta sequence
dropout: 0.2
layer_norm: 0                   # if using layer norm

gpu_id: 7                       #0:3, 1:4, 2:5, 3:7, 4:0, 5:1, 6:2, 7:6
simple: 0